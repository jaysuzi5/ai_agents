{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce59910",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e518321",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# LangGraph Agent\n",
    "This notebook walks through the steps of setting up a simple agent. Although it is basic, it will have the main components including the use of tools and persistent memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9739f0d3",
   "metadata": {},
   "source": [
    "### 1. Load the needed libraries and environment variables\n",
    "Unlike most notebooks, I am not going to load all of the imports at the top, but load them where they are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c4bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af5b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41e526",
   "metadata": {},
   "source": [
    "### 2. Add Tools\n",
    "We will give our agent some access to some tools.  In this case, we are not going to use MCP servers, but instead use LangChain's community tools, which is a robust set of available tools.\n",
    "\n",
    "Additional details on tools:  https://reference.langchain.com/python/langchain/tools/\n",
    "Community resource: https://api.python.langchain.com/en/latest/community/\n",
    "\n",
    "The typical pattern is to find a community resource and then wrap it with a @tool decorator or in this case, create a function that will return a list of tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fbceb7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import Tool\n",
    "from langchain_core.tools import BaseTool\n",
    "from typing import List\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n",
    "from langchain_community.tools.wikipedia.tool import WikipediaQueryRun\n",
    "from langchain_community.agent_toolkits import FileManagementToolkit\n",
    "\n",
    "\n",
    "def get_tools() -> List[BaseTool]:\n",
    "    # Define server, wrapper around the Google API, as a tool\n",
    "    tool_search: Tool = Tool(\n",
    "        name=\"search\",\n",
    "        func=GoogleSerperAPIWrapper().run,\n",
    "        description=\"Use this tool when you want to get the results of an online web search\"\n",
    "    )\n",
    "\n",
    "    # Define Wikipedia as a search as a tool\n",
    "    wikipedia: WikipediaAPIWrapper = WikipediaAPIWrapper()\n",
    "    wiki_tool: WikipediaQueryRun = WikipediaQueryRun(api_wrapper=wikipedia)\n",
    "\n",
    "    # Define file management tools\n",
    "    file_tools: List[BaseTool] = FileManagementToolkit(root_dir=\"sandbox\").get_tools()\n",
    "\n",
    "\n",
    "    # Concatenate all tools together as a list\n",
    "    tools: List[BaseTool] = file_tools + [tool_search, wiki_tool]\n",
    "\n",
    "    return tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d043a674",
   "metadata": {},
   "source": [
    "### 3. Setup Memory\n",
    "LangGraph makes it easy to have persistent memory, but in order be able to retain this memory, we need to define a place to put it.  In this case, we will use SQLite as it is fairly robust for this type of use case.  When building are graph, we will reference this memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "db_path = \"memory.db\"\n",
    "conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "sql_memory = SqliteSaver(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79179fae",
   "metadata": {},
   "source": [
    "### 4. Build the Graph\n",
    "It is now time to put start building the graph.  All graphs include the following steps:\n",
    "<ol>\n",
    "<li>Define the State</li>\n",
    "<li>Start Graph Builder</li>\n",
    "<li>Create Nodes</li>\n",
    "<li>Create Edges</li>\n",
    "<li>Compile the Graph</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# Step 1: Define the State object\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcb4ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# Step 2: Start the Graph Builder with this State class\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4550310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Step 3: Create the nodes.  In this case we have a chatbot and a tool node.\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "tools = get_tools()\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    print(state)\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=tools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02042800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.graph import START\n",
    "\n",
    "# Step 4: Create the edges\n",
    "graph_builder.add_conditional_edges( \"chatbot\", tools_condition, \"tools\")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Step 5: Compile the graph\n",
    "graph = graph_builder.compile(checkpointer=sql_memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032120e",
   "metadata": {},
   "source": [
    "### 5. Test it Out\n",
    "Use Gradio to create a front end that will allow us to test this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio \n",
    "\n",
    "memory_key = \"jay_123\"\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": memory_key}}\n",
    "\n",
    "def chat(user_input: str, history):\n",
    "    result = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config=config)\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "gradio.ChatInterface(chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
