{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce59910",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e518321",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# LangGraph Detailed Agent\n",
    "This advances the agent that was done in the langgraph_agent.ipynb notebook and focuses on defining specific roles to the agents/nodes.  Review that one first to see details on the steps as this one will only explain the additional details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9739f0d3",
   "metadata": {},
   "source": [
    "### 1. Load the needed libraries and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c4bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from typing import List, Any, Optional, Annotated, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "import gradio \n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import BaseTool\n",
    "from langsmith import uuid7\n",
    "from langchain_community.tools import Tool\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n",
    "from langchain_community.tools.wikipedia.tool import WikipediaQueryRun\n",
    "from langchain_community.agent_toolkits import FileManagementToolkit\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af5b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41e526",
   "metadata": {},
   "source": [
    "### 2. Add Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tools() -> List[BaseTool]:\n",
    "    # Define server, wrapper around the Google API, as a tool\n",
    "    tool_search: Tool = Tool(\n",
    "        name=\"search\",\n",
    "        func=GoogleSerperAPIWrapper().run,\n",
    "        description=\"Use this tool when you want to get the results of an online web search\"\n",
    "    )\n",
    "\n",
    "    # Define Wikipedia as a search as a tool\n",
    "    wikipedia: WikipediaAPIWrapper = WikipediaAPIWrapper()\n",
    "    wiki_tool: WikipediaQueryRun = WikipediaQueryRun(api_wrapper=wikipedia)\n",
    "\n",
    "    # Define file management tools\n",
    "    file_tools: List[BaseTool] = FileManagementToolkit(root_dir=\"sandbox\").get_tools()\n",
    "\n",
    "    # Concatenate all tools together as a list\n",
    "    tools: List[BaseTool] = file_tools + [tool_search, wiki_tool]\n",
    "\n",
    "    return tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d043a674",
   "metadata": {},
   "source": [
    "### 3. Setup Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"memory.db\"\n",
    "conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "sql_memory = SqliteSaver(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79179fae",
   "metadata": {},
   "source": [
    "### 4. Build the Graph\n",
    "It is now time to put start building the graph.  All graphs include the following steps:\n",
    "<ol>\n",
    "<li>Define the State</li>\n",
    "<li>Start Graph Builder</li>\n",
    "<li>Create Nodes</li>\n",
    "<li>Create Edges</li>\n",
    "<li>Compile the Graph</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a48ed",
   "metadata": {},
   "source": [
    "### State with additional details\n",
    "Will track additional details in the state to help with routing:\n",
    "<ul>\n",
    "<li>feedback_on_work: details from the evaluator</li>\n",
    "<li>success_criteria_met: determines if it should end</li>\n",
    "<li>number_of_reviews: the number of reviews completed as a guardrail from looping too much</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the State object\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[Any], add_messages]\n",
    "    feedback_on_work: Optional[str]\n",
    "    success_criteria_met: bool\n",
    "    number_of_reviews: int=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcb4ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Start the Graph Builder with this State class\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d351176",
   "metadata": {},
   "source": [
    "### Detailed Agent Setup\n",
    "In this case, we are going to define two agents/nodes with different roles.  The first one will be a \"history\" expert.  The purpose of this agent is that it can do research on historical events and create a detailed report that will be saved as a markdown file.  This will be given a consistent prompt and the user should only have to supply an event.  The second agent/node is an evaluator.  This evaluator will review the report and ensure provide feedback.  Only after it is statified with the results will the process end.  This will also use two different LLMs with the history export using OpenAI and the reviewer using Gemini.  Each of these agents could also be set up with different tool sets as well, however, in this case, I will make the tools available to both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47185838",
   "metadata": {},
   "source": [
    "#### Define the prompts\n",
    "First define a detailed prompt for both agents.  These may very well need to be fined tuned to get them right.  You can use AI to get a jump start on crafting a good system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3bf165",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_expert_system_message = \"\"\"\n",
    "You are HistoryExpert, an autonomous research agent specializing in historical events. You have access to tools including Google Search, Wikipedia, and file management (read_file, write_file, list_directory, delete_file). Your mission is to take a single historical event as input and generate a deep, comprehensive, meticulously accurate historical report about that event, and SAVE IT TO DISK.\n",
    "\n",
    "OUTPUT REQUIREMENTS:\n",
    "\n",
    "1. Begin with a concise executive summary. This summary must contain 3–6 bullet points capturing the essential who, what, when, where, why, and outcome of the event. It must be clearly labeled with the header: \"## Executive Summary\".\n",
    "\n",
    "2. After the summary, produce a detailed, lengthy, deeply researched historical report. Use section headers where appropriate. The report must include background, causes, chronology, major participants, significance, geopolitical impact, cultural or social consequences, and primary sources when available. Cite sources (Wikipedia or Google) in plain text, not with citation IDs or URLs unless required.\n",
    "\n",
    "3. IMPORTANT - WRITE THE REPORT TO DISK: After creating the report content, you MUST use the write_file tool to save it. The filename should follow this exact format:\n",
    "   FILENAME: YYYYMMDDHHmm_<event_name>.md\n",
    "   Where YYYYMMDDHHmm is the current UTC timestamp (use today's date), and <event_name> is the event name in snake_case with no spaces.\n",
    "   \n",
    "   Example: If the event is \"Battle of Hastings\" today is 2024-11-15 at 14:30 UTC, the file should be saved as:\n",
    "   \"20241115143000_battle_of_hastings.md\"\n",
    "\n",
    "4. Tone requirements: Maintain an authoritative, neutral, academic, objective tone. Note when historians disagree. Avoid speculation unless it is explicitly labeled as speculation.\n",
    "\n",
    "BEHAVIOR RULES:\n",
    "- Prefer primary historical sources first, then Wikipedia, then Google.\n",
    "- If the event name is ambiguous, identify any possible interpretations and ask for clarification if absolutely necessary.\n",
    "- The report must always be highly detailed unless explicitly instructed otherwise.\n",
    "- Do not produce a summary at the bottom; only at the top.\n",
    "- ALWAYS save the file using the write_file tool when done - this is mandatory.\n",
    "\n",
    "FINAL WORKFLOW:\n",
    "1. Research the event using search and wikipedia tools if needed\n",
    "2. Create the full report content (Executive Summary + detailed report)\n",
    "3. Use write_file tool to save the report with the properly formatted filename\n",
    "4. Confirm the file has been saved\n",
    "\"\"\"\n",
    "\n",
    "evaluator_system_message = \"\"\"\n",
    "You are HistoryReviewAgent, an autonomous quality-control reviewer responsible for evaluating the output generated by the HistoryExpert agent. \n",
    "You must evaluate the historical report strictly.  You are tough but fair, however, you always send initial constructive feedback, typically \n",
    "asking for expanded details.\n",
    "\n",
    "Your responsibilities:\n",
    "\n",
    "1. Evaluate whether the report fully complies with the required structure:\n",
    "   - Contains an \"## Executive Summary\" at the top with 3–6 bullet points.\n",
    "   - Contains a lengthy, detailed historical report covering:\n",
    "       background, causes, chronology, major participants, significance,\n",
    "       geopolitical impact, cultural/social consequences, and sources.\n",
    "   - Uses section headers appropriately.\n",
    "   - Maintains an objective, academic, historically accurate tone.\n",
    "   - Avoids speculation unless explicitly labeled as such.\n",
    "   - Has been saved to disk as a file (use read_file tool to verify the file exists in the sandbox directory).\n",
    "\n",
    "2. If ALL success criteria are met:\n",
    "   - Set success_criteria_met = true\n",
    "   - Set feedback to a short confirmation message such as:\n",
    "       \"Report meets all criteria and has been saved to disk.\"\n",
    "   - You should NOT update the number_of_reviews, except to check if it is equal to 0 (i.e. the first review) \n",
    "\n",
    "3. If ANY criteria are NOT met:\n",
    "   - Set success_criteria_met = false\n",
    "   - Set feedback to clear, constructive, actionable correction notes that the HistoryExpert can use.\n",
    "   - You should NOT update the number_of_reviews.\n",
    "\n",
    "IMPORTANT:\n",
    "- You will ALWAYS success_criteria_met = false on the first 3 reviews (i.e. number_of_reviews <= 3) and ask for expanded details.\n",
    "- The evaluator MUST NOT rewrite or fix the report — only evaluate it.\n",
    "- The evaluator MUST include a file name in its response.\n",
    "- Use list_directory and read_file tools to verify the file was actually written to disk.\n",
    "- The only output should be the structured fields required by EvaluatorOutput.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a9631",
   "metadata": {},
   "source": [
    "### Create the History Expert\n",
    "This is similiar to the previous example, but gets a little more complicated on handling the different types of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the history expert\n",
    "\n",
    "open_ai = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "tools_for_history_expert = get_tools()\n",
    "open_ai_with_tools = open_ai.bind_tools(tools_for_history_expert)\n",
    "\n",
    "def history_expert(state: State):\n",
    "    # Convert incoming messages to BaseMessage objects\n",
    "    messages = []\n",
    "    for msg in state[\"messages\"]:\n",
    "        if isinstance(msg, dict):\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                messages.append(SystemMessage(content=msg[\"content\"]))\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                messages.append(AIMessage(content=msg.get(\"content\", \"\"), tool_calls=msg.get(\"tool_calls\")))\n",
    "            elif msg[\"role\"] == \"tool\":\n",
    "                messages.append(ToolMessage(content=msg[\"content\"], tool_call_id=msg.get(\"tool_call_id\")))\n",
    "        else:\n",
    "            messages.append(msg)\n",
    "    # Ensure system message is present\n",
    "    has_system_message = any(isinstance(m, SystemMessage) for m in messages)\n",
    "    if not has_system_message:\n",
    "        messages.insert(0, SystemMessage(content=history_expert_system_message))\n",
    "    results = open_ai_with_tools.invoke(messages)\n",
    "    return {\"messages\": [results]}\n",
    "\n",
    "\n",
    "# Add history_expert and tools_for_history_expert nodes\n",
    "graph_builder.add_node(\"history_expert\", history_expert)\n",
    "graph_builder.add_node(\"tools_for_history_expert\", ToolNode(tools=tools_for_history_expert))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca81592",
   "metadata": {},
   "source": [
    "#### Structured Output\n",
    "For the evaluator, we will define the structure that should be returned.  This will make routing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4550310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the evaluator with structured output\n",
    "class EvaluatorOutput(BaseModel):\n",
    "    feedback: str = Field(description=\"Feedback on the assistant's response\")\n",
    "    success_criteria_met: bool = Field(description=\"Whether the success criteria have been met\")\n",
    "    number_of_reviews: int = Field(description=\"The number of reviews that have been conducted.\")   \n",
    "\n",
    "tools_for_evaluator = get_tools()\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
    ")\n",
    "gemini_with_tools = gemini.bind_tools(tools_for_evaluator)\n",
    "gemini_structured_with_tools = gemini_with_tools.with_structured_output(EvaluatorOutput)\n",
    "\n",
    "\n",
    "def evaluator(state: State):\n",
    "    # Convert incoming messages to BaseMessage objects\n",
    "    messages = []\n",
    "    for msg in state[\"messages\"]:\n",
    "        if isinstance(msg, dict):\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                messages.append(SystemMessage(content=msg[\"content\"]))\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                messages.append(AIMessage(content=msg.get(\"content\", \"\"), tool_calls=msg.get(\"tool_calls\")))\n",
    "            elif msg[\"role\"] == \"tool\":\n",
    "                messages.append(ToolMessage(content=msg[\"content\"], tool_call_id=msg.get(\"tool_call_id\")))\n",
    "        else:\n",
    "            messages.append(msg)\n",
    "    # Ensure system message is present\n",
    "    has_system_message = any(isinstance(m, SystemMessage) for m in messages)\n",
    "    if not has_system_message:\n",
    "        messages.insert(0, SystemMessage(content=evaluator_system_message))\n",
    "\n",
    "    results = gemini_structured_with_tools.invoke(messages)\n",
    "\n",
    "    new_state = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"Evaluator Feedback on this answer: {results.feedback}\",\n",
    "            }\n",
    "        ],\n",
    "        \"feedback_on_work\": results.feedback,\n",
    "        \"success_criteria_met\": results.success_criteria_met,\n",
    "        \"number_of_reviews\": results.number_of_reviews + 1,\n",
    "    }\n",
    "    return new_state\n",
    "\n",
    "\n",
    "# Add evaluator and tools_for_evaluator nodes\n",
    "graph_builder.add_node(\"evaluator\", evaluator)\n",
    "graph_builder.add_node(\"tools_for_evaluator\", ToolNode(tools=tools_for_evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13caf0bb",
   "metadata": {},
   "source": [
    "### Define condition statements.\n",
    "In this case, will not just use the out of the box tool router as we have multiple different tools calls and some more complex flows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d4562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUMBER_OF_REVIEWS = 10\n",
    "\n",
    "def route_history_expert(state: State):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools_for_history_expert\"\n",
    "    return \"evaluator\"\n",
    "\n",
    "\n",
    "def route_evaluator(state: State):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools_for_evaluator\"\n",
    "    if state[\"success_criteria_met\"] or state[\"number_of_reviews\"] >= MAX_NUMBER_OF_REVIEWS:\n",
    "        return \"END\"\n",
    "    else:\n",
    "        return \"history_expert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02042800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create the edges\n",
    "\n",
    "# Entry point\n",
    "graph_builder.add_edge(START, \"history_expert\")\n",
    "\n",
    "# From history_expert: if needs tools, go to tools; otherwise go to evaluator\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"history_expert\", \n",
    "    route_history_expert, \n",
    "    {\"tools_for_history_expert\": \"tools_for_history_expert\", \"evaluator\": \"evaluator\"}\n",
    ")\n",
    "\n",
    "# From tools_for_history_expert: always back to history_expert to process tool results\n",
    "graph_builder.add_edge(\"tools_for_history_expert\", \"history_expert\")\n",
    "\n",
    "# From evaluator: if needs tools, go to tools; otherwise check if done or needs to go back to the history expert\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"evaluator\", \n",
    "    route_evaluator, \n",
    "    {\"tools_for_evaluator\": \"tools_for_evaluator\", \"history_expert\": \"history_expert\", \"END\": END}\n",
    ")\n",
    "\n",
    "# From tools_for_evaluator: always back to evaluator to process tool results  \n",
    "graph_builder.add_edge(\"tools_for_evaluator\", \"evaluator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compile the graph\n",
    "graph = graph_builder.compile(checkpointer=sql_memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032120e",
   "metadata": {},
   "source": [
    "### 5. Test it Out\n",
    "Use Gradio to create a front end that will allow us to test this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_key = \"jay_123\"\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": uuid7()}}\n",
    "\n",
    "def chat(user_input: str, history):\n",
    "    result = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config=config)\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "gradio.ChatInterface(chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
